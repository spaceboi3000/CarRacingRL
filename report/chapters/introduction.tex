\section{Εισαγωγή}
\subsection{Reinforcement Learning}
Η ενισχυτική μάθηση (Reinforcement Learning, RL) αποτελεί μια περιοχή της τεχνητής νοημοσύνης που εστιάζει στην εκμάθηση συμπεριφορών μέσα από την 
αλληλεπίδραση ενός πράκτορα (agent) με ένα περιβάλλον. Ο πράκτορας επιλέγει ενέργειες έχοντας ως στόχο τη μεγιστοποίηση της συνολικής επιβράβευσης, 
χρησιμοποιώντας εμπειρία που συλλέγεται κατά τη διάρκεια της εκπαίδευσης. Παρά την αποτελεσματικότητά της σε απλά περιβάλλοντα, η ενισχυτική μάθηση 
συχνά δυσκολεύεται σε περιβάλλοντα υψηλής διάστασης, όπου οι παρατηρήσεις (π.χ. εικόνες) περιέχουν μεγάλο όγκο περιττής πληροφορίας.

Η ενισχυτική μάθηση μοντελοποιεί τη διαδικασία λήψης αποφάσεων ως ένα Markov Decision Process (MDP), ορίζοντας:
State (s): Η τρέχουσα κατάσταση του περιβάλλοντος η οποία, στο δικό μας σύστημα περιλαμβάνει ένα ντετερμινιστικό κομμάτι h και ένα στοχαστικό κομματι z. 
Το ντετερμινιστικό κομμάτι εκφράζει την προβλέψιμη δυναμική του συστήματος. Το στοχαστικό κομμάτι αντιπροσωπεύει αβεβαιότητες και επιτρέπει 
την πρόβλεψη πιθανών μελλοντικών σεναρίων.
Action (a): Η ενέργεια που εκτελεί ο πράκτορας, π.χ. επιτάχυνση ή στροφή του αυτοκινήτου.
Reward (r): Η επιβράβευση που λαμβάνει ο πράκτορας για κάθε ενέργεια.
Policy (π): Στρατηγική επιλογής ενεργειών βάσει της κατάστασης.
Value function (V): Εκτιμά το συνολικό αναμενόμενο reward από μία κατάσταση.
Στόχος του πράκτορα είναι η μεγιστοποίηση του συνολικού αναμενόμενου reward μέσω της εκπαίδευσης της policy του.

\subsection{Google Dreamer}
Ο αλγόριθμος του  Google Dreamer V1 που προτάθηκε από τους Hafner et al. το 2020 αποτελεί μία σύγχρονη προσέγγιση ενισχυτικής μάθησης που προσπαθεί 
να αντιμετωπίσει αυτές τις προκλήσεις. Το Dreamer παίρνει έμπνευση από τα μοντέλα λανθάνοντος χώρου και τα συνδυάζει με actor-critic πολιτικές, 
επιτρέποντας στον agent να ονειρεύεται μελλοντικές καταστάσεις και να εκπαιδεύεται σε έναν λανθάνοντα χώρο αντί σε ακριβείς πραγματικές παρατηρήσεις. 
Αυτό επιτρέπει την ταχύτερη και λιγότερο κοστοβόρα εκπαίδευση αποφεύγοντας την επανεκκίνηση του χώρου προσομοίωσης σε κάθε βήμα. 
Το οποίο είναι ιδιαίτερα χρήσιμο σε περιβάλλοντα με υψηλές διαστάσεις όπως τα παιχνίδια με εικόνες από τις οποίες πρέπει να 
εξάγουμε μόνο την χρήσιμη πληροφορία.
Στην παρούσα εργασία υλοποιούμε από την αρχή και αναλύουμε το google dreamer V1 στο περιβάλλον CarRacing-V3 του gymnasium. 
Στόχος είναι η εκπαίδευση ενός πράκτορα που ελέγχει το αυτοκίνητο ώστε να παραμένει εντός του δρόμου για όσο το δυνατόν περισσότερο χρόνο.




\subsection{Model-Based vs Model-Free RL}
Σε αντίθεση με αλγορίθμους Model-Free (όπως PPO, DQN) που μαθαίνουν απευθείας από 
την αλληλεπίδραση, το Dreamer (Model-Based) προσφέρει:
\begin{itemize}
    \item \textbf{Sample Efficiency:} Χρειάζεται λιγότερα βήματα στο περιβάλλον, καθώς 
    εκπαιδεύεται χιλιάδες φορές πάνω στα ίδια δεδομένα εντός της "φαντασίας" του.
    \item \textbf{Long-horizon Prediction:} Μέσω του RSSM, μπορεί να προβλέψει τις 
    συνέπειες ενεργειών πολύ πιο μακριά στο μέλλον από ό,τι ένας απλός Q-learning αλγόριθμος.
\end{itemize}
